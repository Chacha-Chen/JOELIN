{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "from html import unescape\n",
    "import re\n",
    "\n",
    "from pprint import pprint\n",
    "from tabulate import tabulate\n",
    "\n",
    "from loadData import loadData\n",
    "import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./vocab.txt', './special_tokens_map.json', './added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = '.'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": [\"<E>\", \"</E>\", \"<URL>\", \"@USER\"]})\n",
    "tokenizer.add_tokens([\"<E>\", \"</E>\", \"<URL>\", \"@USER\"])\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size Report: 26621 / 0 / 0 (train/dev/test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size Report: 13581 / 0 / 0 (train/dev/test)\n",
      "Dataset Size Report: 15165 / 0 / 0 (train/dev/test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size Report: 15487 / 0 / 0 (train/dev/test)\n",
      "Dataset Size Report: 12189 / 0 / 0 (train/dev/test)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader_for_event = {}\n",
    "subtask_list_for_event = {}\n",
    "\n",
    "for event in const.EVENT_LIST:\n",
    "    train_dataloader, dev_dataloader, test_dataloader, subtask_list = loadData(\n",
    "        event, tokenizer, input_text_processing_func_list=[], train_ratio = 1)\n",
    "    train_dataloader_for_event[event] = train_dataloader\n",
    "    subtask_list_for_event[event] = subtask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gold Chunk Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this note, I am check if `gold_chunk_list` need any further processing, such as demoji, unescape and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold_chunk_set_for_subtask_for_event = {}\n",
    "for event in const.EVENT_LIST:\n",
    "    gold_chunk_set_for_subtask = {}\n",
    "    for batch in train_dataloader_for_event[event]:\n",
    "        for batch_data in batch['batch_data']:\n",
    "            subtask_label_dict = batch_data[1]\n",
    "            for subtask, (gold_chunk_list, gold_label) in subtask_label_dict.items():\n",
    "            \n",
    "                gold_chunk_set_for_subtask.setdefault(subtask, set())\n",
    "                gold_chunk_set_for_subtask[subtask].update(gold_chunk_list)\n",
    "\n",
    "    gold_chunk_set_for_subtask_for_event[event] = gold_chunk_set_for_subtask\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stats of Unique Gold Chunk for each subtask at each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for event positive\n",
      "Subtask          Unique Gold Chunk\n",
      "-------------  -------------------\n",
      "age                             62\n",
      "close_contact                  115\n",
      "employer                       221\n",
      "gender_male                    253\n",
      "gender_female                  146\n",
      "name                          1374\n",
      "recent_travel                  114\n",
      "relation                        79\n",
      "when                            44\n",
      "where                          424\n",
      "\n",
      "Stats for event negative\n",
      "Subtask          Unique Gold Chunk\n",
      "-------------  -------------------\n",
      "where                          107\n",
      "age                             12\n",
      "close_contact                   22\n",
      "gender_male                    119\n",
      "gender_female                   62\n",
      "how_long                        22\n",
      "name                           356\n",
      "relation                        65\n",
      "when                            21\n",
      "\n",
      "Stats for event can_not_test\n",
      "Subtask      Unique Gold Chunk\n",
      "---------  -------------------\n",
      "relation                   110\n",
      "symptoms                   125\n",
      "name                       215\n",
      "when                        11\n",
      "where                       88\n",
      "\n",
      "Stats for event death\n",
      "Subtask      Unique Gold Chunk\n",
      "---------  -------------------\n",
      "age                        106\n",
      "name                       436\n",
      "relation                    54\n",
      "symptoms                    12\n",
      "when                        59\n",
      "where                      165\n",
      "\n",
      "Stats for event cure_and_prevention\n",
      "Subtask      Unique Gold Chunk\n",
      "---------  -------------------\n",
      "opinion                      1\n",
      "what_cure                  493\n",
      "who_cure                   128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for event in const.EVENT_LIST:\n",
    "    print(f'Stats for event {event}')\n",
    "    print(tabulate([(subtask, len(gold_chunk_set))\n",
    "                    for subtask, gold_chunk_set in gold_chunk_set_for_subtask_for_event[event].items()],\n",
    "                   headers=['Subtask', 'Unique Gold Chunk']))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check chunks that change after preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note this part may need to be done together with the input text to check if the results match each other.\n",
    "(TODO Think about what's the influence of gold chunk. Maybe irrelevant once we have the correct gold label and process the input text all in the same form)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "GoldChunkAnalysis.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
